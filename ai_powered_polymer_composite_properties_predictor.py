# -*- coding: utf-8 -*-
"""AI-Powered Polymer Composite Properties Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E6WU2lC8YgGkNmMvX1cjfkmWxOAgwCWj

# Generate Synthetic Polymer Composite Data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import joblib
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

def generate_synthetic_polymer_composite_data(n_samples=500):
    """
    Generate synthetic dataset for polymer composite materials with natural biogenic fillers
    """

    # Define material types
    polymer_matrices = ['Epoxy', 'Polyester', 'Vinyl Ester', 'Phenolic', 'Polyurethane']
    biogenic_fillers = ['Bovine Bone Particles', 'Hydroxyapatite', 'Bamboo Fibers',
                       'Wood Flour', 'Rice Husk', 'Coconut Coir', 'Jute Fibers',
                       'Flax Fibers', 'Hemp Fibers', 'Chitosan Particles']

    # Generate base data
    data = []

    for i in range(n_samples):
        # Material composition
        polymer_matrix = np.random.choice(polymer_matrices)
        filler_type = np.random.choice(biogenic_fillers)

        # Mixing ratio (filler weight percentage: 0-50%)
        filler_ratio = np.random.uniform(0, 50)
        matrix_ratio = 100 - filler_ratio

        # Processing parameters
        curing_temp = np.random.uniform(60, 180)  # °C
        curing_time = np.random.uniform(2, 24)   # hours
        pressure = np.random.uniform(0.1, 10)    # MPa

        # Particle size (for particulate fillers)
        particle_size = np.random.uniform(10, 500)  # μm

        # Generate realistic property relationships
        # Base properties for different polymer matrices
        base_properties = {
            'Epoxy': {'tensile': 70, 'flexural': 120, 'impact': 15, 'thermal_cond': 0.2, 'electrical_res': 1e14},
            'Polyester': {'tensile': 50, 'flexural': 90, 'impact': 12, 'thermal_cond': 0.15, 'electrical_res': 1e13},
            'Vinyl Ester': {'tensile': 80, 'flexural': 130, 'impact': 18, 'thermal_cond': 0.18, 'electrical_res': 1e13},
            'Phenolic': {'tensile': 45, 'flexural': 85, 'impact': 8, 'thermal_cond': 0.25, 'electrical_res': 1e12},
            'Polyurethane': {'tensile': 35, 'flexural': 60, 'impact': 25, 'thermal_cond': 0.3, 'electrical_res': 1e11}
        }

        # Filler effect factors
        filler_effects = {
            'Bovine Bone Particles': {'tensile': 1.4, 'flexural': 1.3, 'impact': 0.8, 'thermal_cond': 2.0, 'electrical_res': 0.1},
            'Hydroxyapatite': {'tensile': 1.5, 'flexural': 1.4, 'impact': 0.7, 'thermal_cond': 2.5, 'electrical_res': 0.05},
            'Bamboo Fibers': {'tensile': 1.8, 'flexural': 1.6, 'impact': 1.2, 'thermal_cond': 1.2, 'electrical_res': 0.8},
            'Wood Flour': {'tensile': 1.2, 'flexural': 1.1, 'impact': 0.9, 'thermal_cond': 1.5, 'electrical_res': 0.5},
            'Rice Husk': {'tensile': 1.1, 'flexural': 1.0, 'impact': 0.8, 'thermal_cond': 1.3, 'electrical_res': 0.3},
            'Coconut Coir': {'tensile': 1.3, 'flexural': 1.2, 'impact': 1.1, 'thermal_cond': 1.1, 'electrical_res': 0.7},
            'Jute Fibers': {'tensile': 1.7, 'flexural': 1.5, 'impact': 1.3, 'thermal_cond': 1.2, 'electrical_res': 0.6},
            'Flax Fibers': {'tensile': 1.9, 'flexural': 1.7, 'impact': 1.4, 'thermal_cond': 1.3, 'electrical_res': 0.4},
            'Hemp Fibers': {'tensile': 1.6, 'flexural': 1.4, 'impact': 1.2, 'thermal_cond': 1.1, 'electrical_res': 0.5},
            'Chitosan Particles': {'tensile': 1.1, 'flexural': 1.0, 'impact': 0.9, 'thermal_cond': 1.4, 'electrical_res': 0.2}
        }

        base_props = base_properties[polymer_matrix]
        filler_props = filler_effects[filler_type]

        # Calculate properties with filler effects
        filler_factor = filler_ratio / 100

        # Mechanical Properties
        tensile_strength = base_props['tensile'] * (1 + filler_factor * (filler_props['tensile'] - 1))
        flexural_strength = base_props['flexural'] * (1 + filler_factor * (filler_props['flexural'] - 1))
        impact_strength = base_props['impact'] * (1 + filler_factor * (filler_props['impact'] - 1))

        # Add process effects
        temp_effect = 1 + (curing_temp - 120) * 0.002
        time_effect = 1 + (curing_time - 8) * 0.01
        pressure_effect = 1 + (pressure - 1) * 0.05

        tensile_strength *= temp_effect * time_effect * pressure_effect
        flexural_strength *= temp_effect * time_effect * pressure_effect
        impact_strength *= temp_effect * time_effect * pressure_effect

        # Thermal Properties
        thermal_conductivity = base_props['thermal_cond'] * (1 + filler_factor * (filler_props['thermal_cond'] - 1))
        glass_transition_temp = np.random.uniform(50, 200)  # °C
        thermal_expansion = np.random.uniform(20, 80)  # ppm/°C

        # Electrical Properties
        electrical_resistivity = base_props['electrical_res'] * (1 + filler_factor * (filler_props['electrical_res'] - 1))
        dielectric_constant = np.random.uniform(2.5, 8.0)
        dielectric_strength = np.random.uniform(15, 50)  # kV/mm

        # Add realistic noise
        noise_factor = 0.1
        tensile_strength += np.random.normal(0, tensile_strength * noise_factor)
        flexural_strength += np.random.normal(0, flexural_strength * noise_factor)
        impact_strength += np.random.normal(0, impact_strength * noise_factor)
        thermal_conductivity += np.random.normal(0, thermal_conductivity * noise_factor)
        electrical_resistivity += np.random.normal(0, electrical_resistivity * noise_factor)

        # Ensure positive values
        tensile_strength = max(tensile_strength, 5)
        flexural_strength = max(flexural_strength, 5)
        impact_strength = max(impact_strength, 1)
        thermal_conductivity = max(thermal_conductivity, 0.05)
        electrical_resistivity = max(electrical_resistivity, 1e8)

        # Density calculation
        density = 1.2 + filler_factor * 0.8  # g/cm³

        data.append({
            'Polymer_Matrix': polymer_matrix,
            'Filler_Type': filler_type,
            'Filler_Ratio_wt%': filler_ratio,
            'Matrix_Ratio_wt%': matrix_ratio,
            'Curing_Temperature_C': curing_temp,
            'Curing_Time_hours': curing_time,
            'Pressure_MPa': pressure,
            'Particle_Size_microns': particle_size,
            'Density_g_cm3': density,
            'Tensile_Strength_MPa': tensile_strength,
            'Flexural_Strength_MPa': flexural_strength,
            'Impact_Strength_J_m': impact_strength,
            'Thermal_Conductivity_W_mK': thermal_conductivity,
            'Glass_Transition_Temp_C': glass_transition_temp,
            'Thermal_Expansion_ppm_C': thermal_expansion,
            'Electrical_Resistivity_Ohm_m': electrical_resistivity,
            'Dielectric_Constant': dielectric_constant,
            'Dielectric_Strength_kV_mm': dielectric_strength
        })

    return pd.DataFrame(data)

# Generate the dataset
print("Generating synthetic polymer composite dataset...")
df = generate_synthetic_polymer_composite_data(500)

# Display basic information
print(f"\nDataset shape: {df.shape}")
print(f"\nFirst 5 rows:")
print(df.head())

print(f"\nDataset info:")
print(df.info())

print(f"\nBasic statistics:")
print(df.describe())

# Check for missing values
print(f"\nMissing values:")
print(df.isnull().sum())

# Save the dataset
df.to_csv('polymer_composite_dataset.csv', index=False)
print("\nDataset saved as 'polymer_composite_dataset.csv'")

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Distribution of polymer matrices
df['Polymer_Matrix'].value_counts().plot(kind='bar', ax=axes[0,0])
axes[0,0].set_title('Distribution of Polymer Matrices')
axes[0,0].set_xlabel('Polymer Matrix')
axes[0,0].set_ylabel('Count')

# Distribution of filler types
df['Filler_Type'].value_counts().plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('Distribution of Filler Types')
axes[0,1].set_xlabel('Filler Type')
axes[0,1].set_ylabel('Count')
axes[0,1].tick_params(axis='x', rotation=45)

# Filler ratio distribution
df['Filler_Ratio_wt%'].hist(bins=20, ax=axes[1,0])
axes[1,0].set_title('Distribution of Filler Ratios')
axes[1,0].set_xlabel('Filler Ratio (wt%)')
axes[1,0].set_ylabel('Frequency')

# Tensile strength vs filler ratio
axes[1,1].scatter(df['Filler_Ratio_wt%'], df['Tensile_Strength_MPa'], alpha=0.6)
axes[1,1].set_title('Tensile Strength vs Filler Ratio')
axes[1,1].set_xlabel('Filler Ratio (wt%)')
axes[1,1].set_ylabel('Tensile Strength (MPa)')

plt.tight_layout()
plt.savefig('dataset_visualization.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nDataset generation completed successfully!")

"""# ML Model Trainer for Polymer Composite Properties"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler, LabelEncoder
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class PolymerCompositePredictor:
    def __init__(self):
        self.model = None
        self.scaler = None
        self.label_encoders = {}
        self.feature_names = None
        self.target_names = None

    def preprocess_data(self, df):
        """
        Preprocess the dataset for machine learning
        """
        # Make a copy to avoid modifying original data
        data = df.copy()

        # Define categorical and numerical features
        categorical_features = ['Polymer_Matrix', 'Filler_Type']
        numerical_features = ['Filler_Ratio_wt%', 'Matrix_Ratio_wt%', 'Curing_Temperature_C',
                             'Curing_Time_hours', 'Pressure_MPa', 'Particle_Size_microns', 'Density_g_cm3']

        # Define target variables (properties to predict)
        target_features = ['Tensile_Strength_MPa', 'Flexural_Strength_MPa', 'Impact_Strength_J_m',
                          'Thermal_Conductivity_W_mK', 'Glass_Transition_Temp_C', 'Thermal_Expansion_ppm_C',
                          'Electrical_Resistivity_Ohm_m', 'Dielectric_Constant', 'Dielectric_Strength_kV_mm']

        # Encode categorical variables
        for feature in categorical_features:
            if feature not in self.label_encoders:
                self.label_encoders[feature] = LabelEncoder()
                data[feature] = self.label_encoders[feature].fit_transform(data[feature])
            else:
                data[feature] = self.label_encoders[feature].transform(data[feature])

        # Prepare features and targets
        X = data[categorical_features + numerical_features]
        y = data[target_features]

        # Handle log transformation for electrical resistivity
        y_processed = y.copy()
        y_processed['Electrical_Resistivity_Ohm_m'] = np.log10(y_processed['Electrical_Resistivity_Ohm_m'])

        self.feature_names = X.columns.tolist()
        self.target_names = y_processed.columns.tolist()

        return X, y_processed

    def train_model(self, X, y):
        """
        Train the multi-output regression model
        """
        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # Scale the features
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # Create and train the model
        base_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )

        self.model = MultiOutputRegressor(base_model)
        self.model.fit(X_train_scaled, y_train)

        # Make predictions
        y_pred_train = self.model.predict(X_train_scaled)
        y_pred_test = self.model.predict(X_test_scaled)

        # Calculate metrics
        train_metrics = self.calculate_metrics(y_train, y_pred_train)
        test_metrics = self.calculate_metrics(y_test, y_pred_test)

        return (X_train, X_test, y_train, y_test,
                y_pred_train, y_pred_test,
                train_metrics, test_metrics)

    def calculate_metrics(self, y_true, y_pred):
        """
        Calculate regression metrics for multi-output model
        """
        metrics = {}

        for i, target in enumerate(self.target_names):
            metrics[target] = {
                'R2': r2_score(y_true.iloc[:, i], y_pred[:, i]),
                'RMSE': np.sqrt(mean_squared_error(y_true.iloc[:, i], y_pred[:, i])),
                'MAE': mean_absolute_error(y_true.iloc[:, i], y_pred[:, i])
            }

        # Overall metrics
        metrics['Overall'] = {
            'R2': r2_score(y_true, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
            'MAE': mean_absolute_error(y_true, y_pred)
        }

        return metrics

    def predict(self, X):
        """
        Make predictions on new data
        """
        if self.model is None or self.scaler is None:
            raise ValueError("Model must be trained first")

        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)

        # Convert electrical resistivity back from log scale
        predictions_df = pd.DataFrame(predictions, columns=self.target_names)
        predictions_df['Electrical_Resistivity_Ohm_m'] = 10 ** predictions_df['Electrical_Resistivity_Ohm_m']

        return predictions_df

    def get_feature_importance(self):
        """
        Get feature importance for each target variable
        """
        if self.model is None:
            raise ValueError("Model must be trained first")

        importance_dict = {}

        for i, target in enumerate(self.target_names):
            importance_dict[target] = dict(zip(
                self.feature_names,
                self.model.estimators_[i].feature_importances_
            ))

        return importance_dict

    def save_model(self, filepath):
        """
        Save the trained model and preprocessors
        """
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'feature_names': self.feature_names,
            'target_names': self.target_names
        }
        joblib.dump(model_data, filepath)
        print(f"Model saved to {filepath}")

    def load_model(self, filepath):
        """
        Load a trained model and preprocessors
        """
        model_data = joblib.load(filepath)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.label_encoders = model_data['label_encoders']
        self.feature_names = model_data['feature_names']
        self.target_names = model_data['target_names']
        print(f"Model loaded from {filepath}")

# Main training script
def main():
    # Load the dataset
    print("Loading dataset...")
    df = pd.read_csv('polymer_composite_dataset.csv')

    # Initialize the predictor
    predictor = PolymerCompositePredictor()

    # Preprocess the data
    print("Preprocessing data...")
    X, y = predictor.preprocess_data(df)

    # Train the model
    print("Training model...")
    (X_train, X_test, y_train, y_test,
     y_pred_train, y_pred_test,
     train_metrics, test_metrics) = predictor.train_model(X, y)

    # Print results
    print("\n" + "="*50)
    print("MODEL PERFORMANCE METRICS")
    print("="*50)

    print("\nTraining Metrics:")
    for target, metrics in train_metrics.items():
        if target != 'Overall':
            print(f"{target}:")
            print(f"  R² Score: {metrics['R2']:.4f}")
            print(f"  RMSE: {metrics['RMSE']:.4f}")
            print(f"  MAE: {metrics['MAE']:.4f}")

    print(f"\nOverall Training R² Score: {train_metrics['Overall']['R2']:.4f}")

    print("\nTest Metrics:")
    for target, metrics in test_metrics.items():
        if target != 'Overall':
            print(f"{target}:")
            print(f"  R² Score: {metrics['R2']:.4f}")
            print(f"  RMSE: {metrics['RMSE']:.4f}")
            print(f"  MAE: {metrics['MAE']:.4f}")

    print(f"\nOverall Test R² Score: {test_metrics['Overall']['R2']:.4f}")

    # Feature importance
    print("\n" + "="*50)
    print("FEATURE IMPORTANCE")
    print("="*50)

    feature_importance = predictor.get_feature_importance()

    # Create feature importance visualization
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    axes = axes.flatten()

    for i, (target, importance) in enumerate(feature_importance.items()):
        if i < 9:  # We have 9 targets
            features = list(importance.keys())
            importances = list(importance.values())

            # Sort by importance
            sorted_idx = np.argsort(importances)[::-1]

            axes[i].bar(range(len(features)), [importances[j] for j in sorted_idx])
            axes[i].set_title(f'Feature Importance - {target}')
            axes[i].set_xticks(range(len(features)))
            axes[i].set_xticklabels([features[j] for j in sorted_idx], rotation=45)

    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Create prediction vs actual plots
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    axes = axes.flatten()

    for i, target in enumerate(predictor.target_names):
        if i < 9:
            y_true = y_test.iloc[:, i]
            y_pred = y_pred_test[:, i]

            axes[i].scatter(y_true, y_pred, alpha=0.6)
            axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
            axes[i].set_xlabel(f'Actual {target}')
            axes[i].set_ylabel(f'Predicted {target}')
            axes[i].set_title(f'{target}\nR² = {test_metrics[target]["R2"]:.3f}')

    plt.tight_layout()
    plt.savefig('prediction_vs_actual.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Save the model
    predictor.save_model('polymer_composite_model.pkl')

    # Example prediction
    print("\n" + "="*50)
    print("EXAMPLE PREDICTION")
    print("="*50)

    # Create example input
    example_input = pd.DataFrame({
        'Polymer_Matrix': ['Epoxy'],
        'Filler_Type': ['Bovine Bone Particles'],
        'Filler_Ratio_wt%': [30.0],
        'Matrix_Ratio_wt%': [70.0],
        'Curing_Temperature_C': [120.0],
        'Curing_Time_hours': [8.0],
        'Pressure_MPa': [2.0],
        'Particle_Size_microns': [100.0],
        'Density_g_cm3': [1.5]
    })

    # Encode categorical variables for prediction
    for feature in ['Polymer_Matrix', 'Filler_Type']:
        example_input[feature] = predictor.label_encoders[feature].transform(example_input[feature])

    # Make prediction
    prediction = predictor.predict(example_input)

    print("Input:")
    print("- Polymer Matrix: Epoxy")
    print("- Filler Type: Bovine Bone Particles")
    print("- Filler Ratio: 30.0 wt%")
    print("- Curing Temperature: 120.0°C")
    print("- Curing Time: 8.0 hours")
    print("- Pressure: 2.0 MPa")
    print("- Particle Size: 100.0 μm")
    print("- Density: 1.5 g/cm³")

    print("\nPredicted Properties:")
    for target, value in zip(predictor.target_names, prediction.iloc[0]):
        if target == 'Electrical_Resistivity_Ohm_m':
            print(f"- {target}: {value:.2e}")
        else:
            print(f"- {target}: {value:.2f}")

    return predictor

if __name__ == "__main__":
    predictor = main()
    print("\nModel training completed successfully!")